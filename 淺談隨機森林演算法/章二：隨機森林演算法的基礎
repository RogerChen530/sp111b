2.1 決策樹的基本原理

決策樹是一種基於樹狀結構的分類和回歸方法。
它通過一系列的問題和判斷來對數據進行分類或預測。
每個決策樹由節點（代表問題或判斷）、分支（代表可能的答案或結果）和葉子節點（代表類別或數值預測）組成。
決策樹的建立過程包括選擇最佳的問題或判斷點、分裂數據集、遞歸地構建子樹等步驟。

2.2 隨機森林的建立過程

隨機森林的建立過程包括以下步驟：
1.隨機抽樣：從訓練數據集中隨機選擇一部分樣本構成一個子樣本集，並重複這個過程多次。
2.特徵選擇：從所有特徵中隨機選擇一部分特徵，用於構建每個決策樹。
3.構建決策樹：對於每個子樣本集，使用選定的特徵構建一個決策樹。
4.集成預測：將每個決策樹的預測結果進行投票（分類問題）或平均化（迴歸問題），得到最終的預測結果。

2.3 隨機森林的參數調整

隨機森林有一些重要的參數需要調整，以達到最佳的性能。以下是一些常用的參數：
1.決策樹數量（n_estimators）：決定隨機森林中決策樹的數量。通常情況下，增加決策樹的數量可以提高模型的準確性，但同時也會增加計算成本。
2.特徵選擇數量（max_features）：控制每個決策樹在構建過程中選擇的特徵數量。較小的max_features值可以增加模型的多樣性，但可能會降低模型的準確性。
3.樹的深度（max_depth）：限制決策樹的深度，防止過度擬合。適當地設置max_depth可以平衡模型的複雜度和準確性。
